{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "muslim-spencer",
   "metadata": {},
   "source": [
    "#  Basic Data Science in Python - Exercises 13/10  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN, KMeans, Birch, OPTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-annex",
   "metadata": {},
   "source": [
    "### Exercise  1: Comparing clustering methods\n",
    "Below you can see the Moons dataset, wich is two half-circles in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons()\n",
    "plt.scatter(*X.T, c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-victoria",
   "metadata": {},
   "source": [
    "Try using both DBScan and K-Means to label the two half-circles. Which method creates the correct clustering? Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_labels = DBSCAN().fit_predict(X)\n",
    "plt.scatter(*X.T, c=db_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-landing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k_labels = KMeans(n_clusters=4).fit_predict(X)\n",
    "plt.scatter(*X.T, c=k_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-change",
   "metadata": {},
   "source": [
    "### Exercise 2: Clustering the Iris Dataset\n",
    "Use different clustering methods to learn a clustering of the iris dataset. Visualize the clusterings, and use Normalized Mutual Information (NMI) to measure which clustering method performs the best on the dataset.\n",
    "\n",
    "Try to tune the hyperparameters to get the best clustering out of each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "from sklearn.metrics import homogeneity_score as purity\n",
    "from sklearn.cluster import Birch, OPTICS\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "plt.scatter(*X.T[2:4], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_labels = DBSCAN(eps=0.4,min_samples=5).fit_predict(X)\n",
    "plt.scatter(*X.T[2:4], c=db_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_labels = KMeans(n_clusters=3).fit_predict(X)\n",
    "plt.scatter(*X.T[2:4], c=k_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_labels = Birch().fit_predict(X)\n",
    "plt.scatter(*X.T[2:4], c=birch_labels)\n",
    "print(birch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "optic_labels = OPTICS(min_samples=20).fit_predict(X)\n",
    "plt.scatter(*X.T[2:4], c=optic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-tobago",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"NMI\")\n",
    "print(\"KMeans:\", nmi(y, k_labels))\n",
    "print(\"DBSCAN:\", nmi(y, db_labels))\n",
    "print(\"BIRCH:\", nmi(y, birch_labels))\n",
    "print(\"OPTICS\", nmi(y, optic_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Purity\")\n",
    "print(\"KMeans:\", purity(y, k_labels))\n",
    "print(\"DBSCAN:\", purity(y, db_labels))\n",
    "print(\"BIRCH:\", purity(y, birch_labels))\n",
    "print(\"OPTICS\", purity(y, optic_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-packaging",
   "metadata": {},
   "source": [
    "### Exercise 3: Different Size Clusters (Handin)\n",
    "\n",
    "Use k-Means to cluster the below dataset. What happens? Which method should you use instead? Use the method you deem most fitting to cluster the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_std = [1.5, 0.5]\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=[1000, 100],\n",
    "    centers=[[0.0, 0.0], [3.5, 3.5]],\n",
    "    cluster_std=clusters_std,\n",
    "    random_state=0,\n",
    "    shuffle=False,\n",
    ")\n",
    "plt.scatter(*X.T, c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-connection",
   "metadata": {},
   "source": [
    "### Exercise 4: Outlier detection\n",
    "Look at the below dataset. What points do you consider outliers? \n",
    "\n",
    "Use Local Outlier Factor to detect the outlier points. How many agrees with your theory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-lender",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=1000, noise=0.2)\n",
    "for i in np.random.randint(0, 100, size=10):\n",
    "    X[i] = X[i] * 2\n",
    "plt.scatter(*X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-peeing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "from sklearn.neighbors import LocalOutlierFactor as LOF\n",
    "\n",
    "lof_outliers = LOF(n_neighbors=25).fit_predict(X)\n",
    "plt.scatter(*X.T, c=lof_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-albany",
   "metadata": {},
   "source": [
    "### Exercise 5: K-Means for Colour Compression\n",
    "An out-of-the box use of k-Means is using it for image compression. Below is an image that we want to compress to 10 colours. Since the colours can be seen as data points, use k-Means to cluster these together, and only use the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "china = datasets.load_sample_image('china.jpg')\n",
    "X = china/255 #Normalize the data\n",
    "X = X.reshape(427*640, 3) #reshape the data\n",
    "plt.imshow(china)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "km = KMeans(n_clusters=100)\n",
    "km.fit(X)\n",
    "new_colours = km.cluster_centers_[km.predict(X)]\n",
    "new_china = new_colours.reshape(china.shape)\n",
    "plt.imshow(new_china)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-lancaster",
   "metadata": {},
   "source": [
    "### Exercise 6: Implement DB-Outliers (Hard)\n",
    "Another algorithm for finding outliers is Distance-Based Outlier Detection. It works by the following formula:\n",
    "$$ OutlierSet(\\varepsilon, \\pi) = \\Big\\{ p \\in X : \\frac{|\\{x\\in X : dist(p, x) < \\varepsilon\\} |}{n} \\leq \\pi \\Big\\} $$\n",
    "That is, a point $p$ is an outlier if at most $\\pi$ percent of $x\\in D$ has a distance of less than $\\varepsilon$ to $p$. \n",
    "\n",
    "Implement a simple Distance Based Outlier Detection algorithm and test it on the below dataset. Try tuning the parameters $\\pi$ and $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_outliers(X, eps, pi):\n",
    "    outlier_set = [0 for _ in range(X.shape[0])]\n",
    "    ### YOUR CODE HERE\n",
    "    for i, p in enumerate(X):\n",
    "        nr_of_points = 0\n",
    "        for x in X:\n",
    "            if np.linalg.norm(p -x)**2 < eps:\n",
    "                nr_of_points += 1\n",
    "        if nr_of_points/X.shape[0] <= pi:\n",
    "            outlier_set[i] = 1\n",
    "    ### YOUR CODE HERE\n",
    "    return outlier_set\n",
    "\n",
    "X, _ = datasets.make_blobs(n_samples=100, centers=1, n_features=2, center_box=(0, 10), cluster_std=0.7)\n",
    "#Add noise:\n",
    "for i in np.random.randint(0, 100, size=10):\n",
    "    X[i] = X[i] * 1.1\n",
    "\n",
    "outliers = db_outliers(X, eps=1, pi=0.1) #Try tuning eps and pi\n",
    "plt.scatter(*X.T, c=outliers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
